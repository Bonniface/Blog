'''
Select the best model for the given dataset
:param X: features
:param Y: labels
:return: the name and the accuracy of the model for the given dataset
'''
set.seed(2022)
XTrain =  Train.drop(['cost_category','Tour_ID'], axis=1)
yTrain =  Train['cost_category']
XTest  =  r.Testdata.drop
yTest  =  r.yTest
model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)
model.fit(XTrain,yTrain)
predictions = model.predict(XTest)
predictions
quit
View(Traindata)
reticulate::repl_python()
View(X_Train)
XTrain =  X_Train.drop(['cost_category','Tour_ID'], axis=1)
quit
from sklearn.linear_model import LogisticRegression, SGDClassifier
reticulate::repl_python()
from sklearn.naive_bayes import GaussianNB
quit
reticulate::repl_python()
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.metrics import precision_score,recall_score
from sklearn.metrics import f1_score
import xgboost as xgb
from sklearn.neural_network import MLPClassifier
#from lightgbm import LGBMClassifier
'''
Select the best model for the given dataset
:param X: features
:param Y: labels
:return: the name and the accuracy of the model for the given dataset
'''
set.seed(2022)
yTrain =  Train['cost_category']
XTest  =  r.Testdata.drop
yTest  =  r.yTest
model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)
model.fit(X_Train,yTrain)
predictions = model.predict(XTest)
predictions
quit
reticulate::repl_python()
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.metrics import precision_score,recall_score
from sklearn.metrics import f1_score
import xgboost as xgb
from sklearn.neural_network import MLPClassifier
#from lightgbm import LGBMClassifier
'''
Select the best model for the given dataset
:param X: features
:param Y: labels
:return: the name and the accuracy of the model for the given dataset
'''
set.seed(2022)
XTrain = r.Traindata
yTrain =  r.Traindata['cost_category']
XTest  =  r.Testdata
yTest  =  r.yTest
model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)
model.fit(XTrain,yTrain)
predictions = model.predict(XTest)
predictions
quit
reticulate::repl_python()
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.metrics import precision_score,recall_score
from sklearn.metrics import f1_score
import xgboost as xgb
from sklearn.neural_network import MLPClassifier
#from lightgbm import LGBMClassifier
'''
Select the best model for the given dataset
:param X: features
:param Y: labels
:return: the name and the accuracy of the model for the given dataset
'''
set.seed(2022)
XTrain = r.Traindata
yTrain =  r.Traindata['cost_category']
XTest  =  r.Testdata
yTest  =  r.yTest
models = [RandomForestClassifier(), KNeighborsClassifier(), SVC(), LogisticRegression(),xgb.XGBClassifier(),GaussianNB(),
SGDClassifier(), DecisionTreeClassifier(),MLPClassifier()]
scores = dict()
for model in models:
model.fit(X_Train,yTrain)
predictions = model.predict(X_Test)
predictions
print(f'model: {str(model)}')
print(f'Accuracy_score: {accuracy_score(yTest,predictions)}')
#model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)
#model.fit(XTrain,yTrain)
#predictions = model.predict(XTest)
#predictions
quit
reticulate::repl_python()
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.metrics import precision_score,recall_score
from sklearn.metrics import f1_score
import xgboost as xgb
from sklearn.neural_network import MLPClassifier
#from lightgbm import LGBMClassifier
'''
Select the best model for the given dataset
:param X: features
:param Y: labels
:return: the name and the accuracy of the model for the given dataset
'''
set.seed(2022)
XTrain = r.Traindata
yTrain =  r.Traindata['cost_category']
XTest  =  r.Testdata
yTest  =  r.yTest
models = [RandomForestClassifier(), KNeighborsClassifier(), SVC(), LogisticRegression(),xgb.XGBClassifier(),GaussianNB(),
SGDClassifier(), DecisionTreeClassifier(),MLPClassifier()]
scores = dict()
for model in models:
model.fit(XTrain,yTrain)
predictions = model.predict(XTest)
predictions
print(f'model: {str(model)}')
print(f'Accuracy_score: {accuracy_score(yTest,predictions)}')
#model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)
#model.fit(XTrain,yTrain)
#predictions = model.predict(XTest)
#predictions
quit
reticulate::repl_python()
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.metrics import precision_score,recall_score
from sklearn.metrics import f1_score
import xgboost as xgb
from sklearn.neural_network import MLPClassifier
#from lightgbm import LGBMClassifier
'''
Select the best model for the given dataset
:param X: features
:param Y: labels
:return: the name and the accuracy of the model for the given dataset
'''
set.seed(2022)
XTrain = r.Traindata
yTrain =  r.Traindata['cost_category']
XTest  =  r.Testdata
yTest  =  r.yTest
models = [RandomForestClassifier(), KNeighborsClassifier(), SVC(), LogisticRegression(),xgb.XGBClassifier(),GaussianNB(),
SGDClassifier(), DecisionTreeClassifier(),MLPClassifier()]
scores = dict()
for model in models:
model.fit(X_Train,yTrain)
predictions = model.predict(XTest)
print(f'model: {str(model)}')
print(f'Accuracy_score: {accuracy_score(yTest,predictions)}')
#model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)
#model.fit(XTrain,yTrain)
#predictions = model.predict(XTest)
#predictions
quit
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(r.Traindata)
X_Train = scaler.transform(r.Traindata)
# apply same transformation to test data
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
pca = PCA().fit(X_Train)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,7,1)
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')
quit
reticulate::repl_python()
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split, KFold, GridSearchCV
from sklearn.metrics import precision_score,recall_score
from sklearn.metrics import f1_score
import xgboost as xgb
from sklearn.neural_network import MLPClassifier
#from lightgbm import LGBMClassifier
'''
Select the best model for the given dataset
:param X: features
:param Y: labels
:return: the name and the accuracy of the model for the given dataset
'''
XTrain = r.Traindata
yTrain =  r.Traindata['cost_category']
XTest  =  r.Testdata
yTest  =  r.yTest
models = [RandomForestClassifier(), KNeighborsClassifier(), SVC(), LogisticRegression(),xgb.XGBClassifier(),GaussianNB(),
SGDClassifier(), DecisionTreeClassifier(),MLPClassifier()]
scores = dict()
for model in models:
model.fit(X_Train,yTrain)
predictions = model.predict(XTest)
print(f'model: {str(model)}')
print(f'Accuracy_score: {accuracy_score(yTest,predictions)}')
#model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)
#model.fit(XTrain,yTrain)
#predictions = model.predict(XTest)
#predictions
View(models)
View(models)
View(models)
View(models)
View(models)
quit
summary(pca)
#| label: load-pkgs
#| code-summary: "Packages"
#| message: false
library(openintro)  # for data
#library(tidyverse)  # for data wrangling and visualization
library(knitr)      # for tables
library(broom)      # for model summary
if(!require("pacman")){install.packages("pacman")}
pacman::p_load(char = c('rgee','reticulate','raster','tidyverse',
'dplyr','sf','mapview','mapeddit','caret','forcats','reticulate',
'rgee', 'remotes', 'magrittr', 'tigris', 'tibble', 'stars',                       'stars',
'st', 'lubridate', 'imputeTS', 'leaflet', 'classInt',                            'RColorBrewer',
'ggplot2', 'googledrive', 'geojsonio', 'ggpubr','cartogram'),
install = F, update = F, character.only = T)
gc()
gc()
#| label: load-pkgs
#| code-summary: "Packages"
#| message: false
library(openintro)  # for data
#library(tidyverse)  # for data wrangling and visualization
library(knitr)      # for tables
library(broom)      # for model summary
if(!require("pacman")){install.packages("pacman")}
pacman::p_load(char = c('rgee','reticulate','raster','tidyverse',
'dplyr','sf','mapview','mapeddit','caret','forcats','reticulate',
'rgee', 'remotes', 'magrittr', 'tigris', 'tibble', 'stars',                       'stars',
'st', 'lubridate', 'imputeTS', 'leaflet', 'classInt',                            'RColorBrewer',
'ggplot2', 'googledrive', 'geojsonio', 'ggpubr','cartogram'),
install = F, update = F, character.only = T)
library(rgee)
library(reticulate)
#ee_install()
ee_check()
ee_Initialize("kalong",drive = TRUE) # initialize GEE,
library(rgee)
library(reticulate)
#ee_install()
ee_check()
ee_Initialize("kalong",drive = TRUE) # initialize GEE,
#this will have you log in to Google Drive
library('sf')
# Load shape file
#setwd("C:/Users/Guy/Documents/GitHub/Artisanal-Mining-In-Ghana-Galamsey/New Regions")
aoi <- read_sf('Ghana shp file/GHA/gadm41_GHA_1.shp')
aoi <- st_transform(aoi, st_crs(4326))
aoi.ee <- st_bbox(aoi) %>%
st_as_sfc() %>%
sf_as_ee() #Converts it to an Earth Engine Object
getQABits <- function(image, qa) {
# Convert binary (character) to decimal (little endian)
qa <- sum(2^(which(rev(unlist(strsplit(as.character(qa), "")) == 1))-1))
# Return a mask band image, giving the qa value.
image$bitwiseAnd(qa)$lt(1)
}
mod.clean <- function(img) {
# Extract the NDVI band
ndvi_values <- img$select("EVI")
# Extract the quality band
ndvi_qa <- img$select("SummaryQA")
# Select pixels to mask
quality_mask <- getQABits(ndvi_qa, "11")
# Mask pixels with value zero.
#0.0001 is the MODIS Scale Factor
ndvi_values$updateMask(quality_mask)$divide(ee$Image$constant(10000))
}
modis.evi <- ee$ImageCollection("MODIS/006/MOD13Q1"
)$filter(ee$Filter$date('2000-01-01','2022-01-01'))$map(mod.clean)
library(tibble)
aoi.proj <- st_transform(aoi, st_crs(2392))
hex <- st_make_grid(x = aoi.proj, cellsize = 17280, square = FALSE) %>%
st_sf() %>%
rowid_to_column('hex_id')
hex <- hex[aoi.proj,]
plot(hex)
#This will take about 30 minutes
if(readline(prompt = "Hit enter to proceed or type 'no' to download the data from G-Drive. ") == "no"){
googledrive::drive_download(file =
googledrive::as_id("https://drive.google.com/drive/folders/1ZnCpYz38ezSU1XG7ixJ2sPg_DX7bO07J?usp=sharing"),overwrite = T)
#https://drive.google.com/drive/folders/1ZnCpYz38ezSU1XG7ixJ2sPg_DX7bO07J?usp=sharing
evi.df <- read.csv("rgee_file_2d44527a3b0e_2022_07_28_15_40_52.csv")
evi.df <- evi.df[,3:ncol(evi.df)]
colnames(evi.df) <- c('hex_id', stringr::str_replace_all(substr(colnames(evi.df[, 2:ncol(evi.df)]), 2, 11), "_", "-")) #Convert dates to unambiguous format
} else {
#This will take about 30 minutes
paste0(system.time(expr = aoi.evi <- ee_extract(x = modis.evi, y = hex["hex_id"], sf = FALSE, scale = 250, fun = ee$Reducer$mean(), via = "drive", quiet = T))/60, " Minutes Elapsed. ")
evi.df <- as.data.frame(aoi.evi)
colnames(evi.df) <- c('hex_id', stringr::str_replace_all(substr(colnames(evi.df[, 2:ncol(evi.df)]), 2, 11), "_", "-"))
write.csv(x = evi.df, file = "~/rgee_file_2d44527a3b0e_2022_07_28_15_40_52.csv")}
setwd("~/Generalized Linear Models")
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
df <- read.csv("Data for GLmM.csv")
tail(df)
library(dplyr)
df <- df%>%
dplyr::select(PN, G, YOD, AOD, EL, DUC, F, NC, CAP)%>%
dplyr::filter(CAP>0)
head(df)
library(psych)
describe(df)
hist(df$CAP, main = "Histogrom Of CAP")
plot(density(df$CAP),main="Density estimate of CAP")
plot(ecdf(df$CAP),main="Empirical cumulative distribution function" )
plot(density(df$CAP),main="Density estimate of CAP")
z.norm<-(df$CAP-mean(df$CAP))/sd(df$CAP) ## standardized data
qqnorm(z.norm) ## drawing the QQplot
abline(glm(CAP~.,data= df),col = "blue")
library(goft)
x <- rlnorm(df$CAP)
grid = seq(0,150,.5)
plot(grid,dlnorm(grid,1,.6),type="l",xlab="x",ylab="f(x)")
lines(density(x),col="red")
legend("topright",c("True Density","Estimate"),lty=1,col=1:2)
library(MASS)
set.seed(1)
exponential <- fitdistr(df$CAP, "exponential")
AIC(exponential)
normal <- fitdistr(df$CAP, "normal")
AIC(normal)
cauchy <- fitdistr(df$CAP, "cauchy")
AIC(cauchy)
logistic <- fitdistr(df$CAP, "logistic")
AIC(logistic)
logistic <- fitdistr(df$CAP, "log-normal")
AIC(logistic)
BIC(logistic)
library(stats)
cor(df,method = "spearman",)
library(fitdistrplus)
descdist(df$CAP)
summary(df)
X  <- df$CAP
set.seed(1234)
summary(NormalDitr <- fitdist(X,"norm", method = "mle"))
plot(NormalDitr)
plot(X, pch = 20)
plotdist(X, histo = TRUE, demp = TRUE)
descdist(X,discrete = FALSE,boot = 500,method = "unbiased")
glm.fit(CAP~.,data = )
install.packages("gamlss")
library(gamlss)
plot(exponential)
fit.weibull <- fitdist(X, distr = "weibull", method = "mle", lower = c(0, 0), start = list(scale = 1, shape = 1))
fit.gamma <- fitdist(X, distr = "gamma", method = "mle", lower = c(0, 0), start = list(scale = 1, shape = 1))
glm.fit(CAP~.,data = X)
fitdist(X, "lognormal","mle")
fitdist(X, "lnorm","mle")
summary(fitdist(X, "lnorm","mle"))
summary(Lognormal <-fitdist(X, "lnorm","mle"))
plot(lognormal)
summary(Lognormal <-fitdist(X, "lnorm","mle"))
plot(Lognormal)
fitdist(X,"weibull","mle")
summary(weibull<-fitdist(X,"weibull","mle"))
plot(weibull)
summary(gamma <- fitdist(X,"gamma","mle"))
summary(gamma <- fitdist(X,"exponential","mle"))
library(gamlss)
fitDist(X)
library(gamlss)
fitDist(X,"LOGNO")
library(gamlss)
fitDist(X,"realplus")
library(gamlss)
fitDist(X,"counts")
library(gamlss)
fitDist(X,"realline")
library(gamlss)
fitDist(X,type="realline")
fitDist(X,
type = c("realline", "realplus", "real0to1"))
fitDist(X,type = c("realline", "realplus", "real0to1"))
fitDist(X,type = ("realplus"))
fitDist(X,type = ("realAll"))
gamlss(CAP~ G + YOD + AOD + EL + DUC + F + NC,data=df)
gamlss(CAP~ G + YOD + AOD + EL + DUC + F + NC,data=df, family = ST3)
ST3 <- gamlss(CAP~ G + YOD + AOD + EL + DUC + F + NC,data=df, family = ST3)
summary(ST3)
plot(ST3)
LOGNO <- gamlss(CAP~ G + YOD + AOD + EL + DUC + F + NC,data=df, family = LOGNO)
summary(LOGNO)
plot(LOGNO)
knitr::opts_chunk$set(cache = TRUE)
library(purrr)
library(leaflet)
library(sf)
library(tibble)
library(ggplot2)
library(googledrive)
library(forecast)
library(lubridate)
library(magrittr)
library(dplyr)
library(geojsonio)
library(rgee)
library(reticulate)
ee_check()
ee_Initialize("kalong",drive = TRUE) # initialize GEE,
ee_check()
ee_Initialize("kalong",drive = TRUE) # initialize GEE,
ee_Initialize("kalong",drive = TRUE) # initialize GEE,
ee_check()
ee_Initialize("kalong",drive = TRUE) # initialize GEE,
#this will have you log in to Google Drive
cc <- read_sf('C:/Users/Guy/Documents/GitHub/Artisanal-Mining-In-Ghana-Galamsey/Codes/Ghana shp file/ROI/new_roi.shp')
aoi <- st_transform(cc, st_crs(4326))
aoi.ee <- st_bbox(aoi) %>%
st_as_sfc() %>%
sf_as_ee() #Converts it to an Earth Engine Object
getQABits <- function(image, qa) {
# Convert binary (character) to decimal (little endian)
qa <- sum(2^(which(rev(unlist(strsplit(as.character(qa), "")) == 1))-1))
# Return a mask band image, giving the qa value.
image$bitwiseAnd(qa)$lt(1)
}
mod.clean <- function(img) {
# Extract the NDVI band
ndvi_values <- img$select("EVI")
# Extract the quality band
ndvi_qa <- img$select("SummaryQA")
# Select pixels to mask
quality_mask <- getQABits(ndvi_qa, "11")
# Mask pixels with value zero.
ndvi_values$updateMask(quality_mask)$divide(ee$Image$constant(10000)) #0.0001 is the MODIS Scale Factor
}
modis.evi <- ee$ImageCollection("MODIS/006/MOD13Q1")$filter(ee$Filter$date('2000-01-01', '2022-01-01'))$map(mod.clean)
remotes::install_github('rstudio/blogdown')
blogdown::new_site(theme = "google/docsy")
setwd("~/GitHub/Bonniface.github.io/config/_default")
